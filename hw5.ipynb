{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATGC0GT3tXqF"
      },
      "source": [
        "# Assignment 5\n",
        "\n",
        "## k-means clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVXfbs0LtXqI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "tfgqSkbPtXqK",
        "outputId": "b5ba501d-7adb-4773-e2e9-ec120d7dd203"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y_true = make_blobs(n_samples=300, centers=4,\n",
        "                       cluster_std=0.60, random_state=0)\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "pRnw4ihqtXqO",
        "outputId": "f40e78c1-bfae-4047-cd12-d4d2c1860d27"
      },
      "outputs": [],
      "source": [
        "def find_clusters_kmeans(X, n_clusters, rseed=2):\n",
        "    rng = np.random.RandomState(rseed)\n",
        "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
        "\n",
        "    #########################################################################\n",
        "    # TODO: Store randomly choosed cluster centers in the centers variable. #\n",
        "    #########################################################################\n",
        "    # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n",
        "\n",
        "\n",
        "    # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n",
        "\n",
        "\n",
        "    while True:\n",
        "        ###################################################################################\n",
        "        # TODO:Implement a clustering algorithm. You MUST implement all of the following: #\n",
        "        #      (1) Assign each training example X to the closest clustetr centers.        #\n",
        "        #        - calculate the sum of squared errors.                                   #\n",
        "        #        - Store the argmin of squared errors in the labels variable.             #\n",
        "        #      (2) Calculate new cluster centers.                                         #\n",
        "        #        - Store the updated centers in the new_centers variable.                 #\n",
        "        #      (3) Check for covergence.                                                  #\n",
        "        #        - Break the loop if the centers variable is the same as new_centers.     #\n",
        "        #        - Ohterwise, updates the centers variable to be the same as new_centers. #\n",
        "        ###################################################################################\n",
        "        # ************* START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ************* #\n",
        "\n",
        "\n",
        "\n",
        "        # ************** END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ************** #\n",
        "\n",
        "    return centers, labels\n",
        "\n",
        "centers, labels = find_clusters_kmeans(X, 4)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
        "            s=50, cmap='viridis');\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liGWo16gtXqP"
      },
      "source": [
        "## Limitations of k-means\n",
        "\n",
        "### Not optimally guaranteed and initialization\n",
        "Under typical circumstances, each repetition of the E-step and M-step will always result in a better estimate of the cluster characteristics. However, it is best to note that, **although the Eâ€“M procedure is guaranteed to improve the result in each step, there is no assurance that it will lead to the global best solution**.\n",
        "\n",
        "The **initialization is important** and particularly bad initialization can sometimes lead to clearly sub-optimal clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "jShGJi1XtXqP",
        "outputId": "30ea6dc5-bc94-4c17-f047-63e120ff8382"
      },
      "outputs": [],
      "source": [
        "centers, labels = find_clusters_kmeans(X, 4, rseed=0)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
        "            s=50, cmap='viridis');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ean3o7ShtXqP"
      },
      "source": [
        "### Number of clusters?\n",
        "\n",
        "A common challenge with k-means is that you must tell it how many clusters you expect.\n",
        "It cannot learn the number of clusters from the data.\n",
        "\n",
        "If we force the k-means to look for 6 clusters instead of 4, it will come back with 6 but they may not be what we are looking for!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "qHU7jq5CtXqQ",
        "outputId": "1948596b-ee0c-454a-faf5-a044b7caf82f"
      },
      "outputs": [],
      "source": [
        "centers, labels = find_clusters_kmeans(X, 6)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
        "            s=50, cmap='viridis');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBpTU4B7tXqQ"
      },
      "source": [
        "### Complicated datasets\n",
        "\n",
        "\n",
        "k-means algorithm will often be ineffective if the clusters have complicated geometries. In particular, the boundaries between k-means clusters will always be linear, which means that it will fail for more complicated boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCmbvk8ItXqQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "Xmoon, y = make_moons(200, noise=.05, random_state=0)\n",
        "plt.scatter(Xmoon[:, 0], Xmoon[:, 1], s=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "e3HcyQhFtXqQ",
        "outputId": "05478888-e9ad-4912-d143-c43d49a82b65"
      },
      "outputs": [],
      "source": [
        "centers, labels = find_clusters_kmeans(Xmoon, 2, rseed=0)\n",
        "plt.scatter(Xmoon[:, 0], Xmoon[:, 1], c=labels,\n",
        "            s=50, cmap='viridis');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUiL9CsqtXqQ"
      },
      "source": [
        "### Fails for non-circular blobs of data\n",
        "\n",
        "In particular, the non-probabilistic nature of k-means and its use of simple distance-from-cluster-center to assign cluster membership leads to poor performance for many real-world situations.\n",
        "\n",
        "Gaussian mixture models (GMMs), can be viewed as an extension of the ideas behind k-means, but can also be a powerful tool for estimation beyond simple clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "Art8AQN_tXqR",
        "outputId": "15bba808-b1da-41e0-e20f-8672b45da315"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "def plot_kmeans(X, n_clusters=4, rseed=0, ax=None):\n",
        "    centers, labels = find_clusters_kmeans(X, 4, rseed=2)\n",
        "\n",
        "    # plot the input data\n",
        "    ax = ax or plt.gca()\n",
        "    ax.axis('equal')\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis', edgecolor='k',zorder=2)\n",
        "\n",
        "    # plot the representation of the KMeans model\n",
        "    radii = [cdist(X[labels == i], [center]).max()\n",
        "             for i, center in enumerate(centers)]\n",
        "    for c, r in zip(centers, radii):\n",
        "        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))\n",
        "\n",
        "plot_kmeans(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5FdYoMytXqR"
      },
      "source": [
        "There appears to be a very slight overlap between the two middle clusters, such that we might not have complete confidence in the cluster assigment of points between them. Unfortunately, the **k-means model has no intrinsic measure of probability or uncertainty** of cluster assignments.\n",
        "\n",
        "For k-means these cluster models must be circular. k-means has **no built-in way of accounting for oblong or elliptical clusters**. So, for example, if we take the same data and transform it, the cluster assignments end up becoming muddled.\n",
        "\n",
        "k-means is not flexible enough to account for this, and tries to force-fit the data into four circular clusters. This results in a mixing of cluster assignments where the resulting circles overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "T0oRpwUXtXqR",
        "outputId": "1782f51c-3dff-4f50-910d-8343fba17d28"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(4)\n",
        "X_stretched = np.dot(X, rng.randn(2, 2))\n",
        "plot_kmeans(X_stretched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwktnRaltXqR"
      },
      "source": [
        "## Gaussian Mixture Models (GMM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcNouvWQ9_kq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal as N # ~ Gaussian distribution\n",
        "\n",
        "def find_gmm(X, n_clusters, max_iter=100):\n",
        "\n",
        "    rng = np.random.RandomState(0)\n",
        "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
        "\n",
        "    #########################################################################\n",
        "    # TODO: Store randomly choosed points in the means variable.            #\n",
        "    #########################################################################\n",
        "    # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n",
        "\n",
        "\n",
        "    # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n",
        "\n",
        "    covariances = [np.cov(X.T) for _ in range(n_clusters)]\n",
        "    p_z = np.full(n_clusters, 1.0 / n_clusters)\n",
        "\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "    #########################################################################################\n",
        "    # TODO:Implement a GMM. You MUST implement all of the following:                        #\n",
        "    #      (1) Implement the E-step of the algorithm.                                       #\n",
        "    #        - calculate likelihoods (use 'N.pdf(X, mean= , cov= ).                         #\n",
        "    #        - calculate responsibilities and store in the responsibilities variable.       #\n",
        "    #      (2) Implement the M-step of the algorithm.                                       #\n",
        "    #        - update the p_z variable.                                                     #\n",
        "    #        - update the means variable.                                                   #\n",
        "    #        - update the covariances variable.                                             #\n",
        "    #      (3) After the loop, store the argmin of responsibilities in the labels variable. #\n",
        "    #########################################################################################\n",
        "    # **************** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) **************** #\n",
        "\n",
        "\n",
        "    # ************** END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ************** #\n",
        "\n",
        "    return labels, means, covariances, p_z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "T8NSApG3tXqS",
        "outputId": "3ab25a2f-d8e2-41c1-cb47-d8e87e8f2c29"
      },
      "outputs": [],
      "source": [
        "labels, means, covariances, weights = find_gmm(X, 4)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "yfYkGFU-tXqS",
        "outputId": "4be91819-d183-4ee2-8365-1c833a14c2a1"
      },
      "outputs": [],
      "source": [
        "from matplotlib.patches import Ellipse\n",
        "\n",
        "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
        "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
        "    ax = ax or plt.gca()\n",
        "\n",
        "    # Convert covariance to principal axes\n",
        "    if covariance.shape == (2, 2):\n",
        "        U, s, Vt = np.linalg.svd(covariance)\n",
        "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
        "        width, height = 2 * np.sqrt(s)\n",
        "    else:\n",
        "        angle = 0\n",
        "        width, height = 2 * np.sqrt(covariance)\n",
        "\n",
        "    # Draw the Ellipse\n",
        "    for nsig in range(1, 4):\n",
        "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
        "                             angle=angle, **kwargs))\n",
        "\n",
        "def plot_gmm(X, n_clusters, label=True, ax=None):\n",
        "    ax = ax or plt.gca()\n",
        "    labels, means, covariances, weights = find_gmm(X, n_clusters)\n",
        "    if label:\n",
        "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
        "    else:\n",
        "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
        "    ax.axis('equal')\n",
        "\n",
        "    w_factor = 0.2 / max(weights)\n",
        "    for pos, covar, w in zip(means, covariances, weights):\n",
        "        draw_ellipse(pos, covar, alpha=w * w_factor)\n",
        "\n",
        "plot_gmm(X_stretched, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R-C3iJEtXqT"
      },
      "source": [
        "### GMM as density estimation and generative model algorithm\n",
        "\n",
        "Though GMM is often categorized as a clustering algorithm, fundamentally it is an algorithm for density estimation. That is to say, the result of a GMM fit to some data is technically not a clustering model, but a generative probabilistic model describing the distribution of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "collapsed": true,
        "id": "heK1l2cotXqT",
        "outputId": "7ea99439-9058-4bbd-d13f-74fa4428a3f1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plot_gmm(Xmoon,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nif1ZLMAtXqY"
      },
      "source": [
        "Here the mixture of 10 Gaussians serves not to find separated clusters of data, but rather to model the overall distribution of the input data. This is a generative model of the distribution, meaning that the GMM gives us the recipe to generate new random data distributed similarly to our input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QjOLF8tktXqY",
        "outputId": "65ebb39e-ec96-4800-99fb-cba94918e6ac"
      },
      "outputs": [],
      "source": [
        "plot_gmm(Xmoon,10, label=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
