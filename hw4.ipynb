{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment4\n",
    "\n",
    "네번째 과제: 'Multi-layer Perceptron (MLP)'\n",
    "## Packages\n",
    "- [numpy](www.numpy.org)는 Python에서 행렬 작업을 위한 기본 패키지입니다.\n",
    "- [matplotlib](http://matplotlib.org)는 Python에서 그래프를 그리는 라이브러리입니다.\n",
    "- [scikit-learn](https://scikit-learn.org/stable/)는 Python기반 머신러닝 분석을 위한 라이브러리입니다.\n",
    "- [pytorch](https://pytorch.org)는 신경망 구축에 사용되는 오픈소스 딥러닝 프레임워크입니다.\n",
    "- [tabulate](https://pypi.org/project/tabulate/)는 Python에서 데이터를 테이블 형식으로 출력해주는 라이브러리입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mplcolors\n",
    "import sklearn.datasets as skdatasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spiral_data(n_points, n_features, n_classes):\n",
    "    np.random.seed(444)\n",
    "    X = np.zeros((n_points*n_classes, n_features))\n",
    "    y = np.zeros((n_points*n_classes,), dtype=np.uint8)\n",
    "    for j in range(n_classes):\n",
    "        i = range(n_points*j, n_points*(j+1))\n",
    "        r = np.linspace(0., 1, n_points) # radius\n",
    "        t = np.linspace(j*4, (j+1)*4, n_points) + np.random.randn(n_points) * 0.2 # theta\n",
    "        X[i] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[i] = j\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=444)\n",
    "\n",
    "def plot_spiral(X, y, x1_mesh=None, x2_mesh=None, x3_mesh=None):\n",
    "    x1, x2 = X[:, 0], X[:, 1]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x1[y==0], x2[y==0], c='tab:blue', marker='o', label='class:0')\n",
    "    plt.scatter(x1[y==1], x2[y==1], c='tab:orange', marker='^', label='class:1')\n",
    "    plt.scatter(x1[y==2], x2[y==2], c='tab:green', marker='x', label='class:2')\n",
    "    if x1_mesh is not None:\n",
    "        cmap = mplcolors.ListedColormap(['tab:blue', 'tab:orange', 'tab:green'])\n",
    "        plt.contourf(x1_mesh, x2_mesh, y_mesh, levels=3, cmap=cmap, alpha=0.3)\n",
    "    plt.title('MLP (Spiral dataset)', fontsize=18, pad=12)\n",
    "    plt.xlabel('x1', fontsize=16, labelpad=12)\n",
    "    plt.ylabel('x2', fontsize=16, labelpad=12)\n",
    "    plt.legend(fontsize=14, loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = generate_spiral_data(n_points=100, n_features=2, n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spiral(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on lineary non-separable dataset!\n",
    "\n",
    "### MLP implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    yhat = None\n",
    "    #########################################################################\n",
    "    # TODO: Implement the softmax function without using explicit loops.    #\n",
    "    # Store the softmax scores in the yhat variable.                        #\n",
    "    #########################################################################\n",
    "    # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n",
    "\n",
    "\n",
    "\n",
    "    # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n",
    "    return yhat\n",
    "\n",
    "def one_hot(x, n_class):\n",
    "\treturn np.eye(n_class)[x]\n",
    "\n",
    "def cross_entropy_loss(yhat, y, n_class):\n",
    "    # clip values to prevent divide by zero\n",
    "    yhat = np.clip(yhat, a_min=1e-7, a_max=1-1e-7)\n",
    "\n",
    "    loss = None\n",
    "    #########################################################################\n",
    "    # TODO: Implement the cross-entropy function. Store the cross-entropy   #\n",
    "    # loss in the loss variable. Note that the yhat is the output of the    #\n",
    "    # softmax function implemented above.                                   #\n",
    "    #########################################################################\n",
    "    # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n",
    "    \n",
    "\n",
    "    # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, is_first=False):\n",
    "        #########################################################################\n",
    "        # TODO: Initialize the weights and biases of the linear layer. Store    #\n",
    "        # the weights in the self.w variable and biases in the self.b variable. #\n",
    "        # Weights should be a 2d np.ndarray of shape (in_features, out_features)#\n",
    "        # and initialized from a Uniform ranged [0, 1]. Biases should be a 1d   #\n",
    "        # np.ndarray of shape (out_features,) and initialized to zero.          #\n",
    "        #########################################################################\n",
    "        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n",
    "        \n",
    "\n",
    "\n",
    "        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        self.is_first = is_first\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out = None\n",
    "        self.input = x # cache for backward\n",
    "        #########################################################################\n",
    "        # TODO: Implement the forward pass for the linear layer. Compute the    #\n",
    "        # affine transform of X and store the result in the out variable.       #\n",
    "        #########################################################################\n",
    "        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n",
    "        \n",
    "\n",
    "        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        #########################################################################\n",
    "        # TODO: Implement the backward pass for the linear layer. Store the     #\n",
    "        # gradient of loss w.r.t weights (dL/dw) in the self.dw variable, and   #\n",
    "        # the gradient of loss w.r.t biases (dL/db) in the self.db variable.    #\n",
    "        #########################################################################\n",
    "        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n",
    "        \n",
    "\n",
    "        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n",
    "\n",
    "        dx = None\n",
    "        #########################################################################\n",
    "        # TODO: Implement the backward pass for the linear layer. Store the     #\n",
    "        # gradient of loss w.r.t inputs (dL/dx) in the dx variable. Note that   #\n",
    "        # we compute dx, if this is not the first layer (i.e., if self.is_first #\n",
    "        # is False)                                                             #\n",
    "        #########################################################################\n",
    "        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n",
    "\n",
    "        \n",
    "        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __call__(self, x):\n",
    "        self.input = x # cache for backward\n",
    "\n",
    "        out = None\n",
    "        #########################################################################\n",
    "        # TODO: Implement the forward pass of ReLU layer. Store the computed    #\n",
    "        # values in the out variable                                            #\n",
    "        #########################################################################\n",
    "        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n",
    "\n",
    "\n",
    "        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = None\n",
    "        #########################################################################\n",
    "        # TODO: Implement the backward pass of ReLU layer. Store the computed   #\n",
    "        # values in the dx variable. Note that you can use self.input to        #\n",
    "        # compute the gradients.                                                #\n",
    "        #########################################################################\n",
    "        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n",
    "\n",
    "\n",
    "        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, features):\n",
    "        def setup_layers(features):\n",
    "            layers = [] # n: in_features, m: out_features\n",
    "            for i, (n, m) in enumerate(zip(features[:-1], features[1:])):\n",
    "                layers += [Linear(n, m, i==0), ReLU()]\n",
    "            del layers[-1]\n",
    "            return layers\n",
    "        \n",
    "        self.layers = setup_layers(features)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self, n_class):\n",
    "        self.n_class = n_class\n",
    "    \n",
    "    def __call__(self, x, y):\n",
    "        loss, grad = None, None\n",
    "        #########################################################################\n",
    "        # TODO: Compute the cross-entropy loss and its gradient without using   #\n",
    "        # explicit loops. Store the loss in loss variable and the gradient in   #\n",
    "        # grad variable. Please use the softmax and cross_entropy_loss function #\n",
    "        # implemented above.                                                    #\n",
    "        #########################################################################\n",
    "        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n",
    "    \n",
    "\n",
    "        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n",
    "        return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    def __init__(self, layers, lr):\n",
    "        self.layers = layers\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        #########################################################################\n",
    "        # TODO: Implement the gradient descent algorithm. Note that self.layers #\n",
    "        # is a list of layers, where each of them could be either Linear or     # \n",
    "        # ReLU. Since a ReLU layer doesn't have learnable parameters, we only   #\n",
    "        # update the parameters of the Linear layers. Loop through self.layers  #\n",
    "        # and update the parameters if the layer is instance of Linear class.   #\n",
    "        #########################################################################\n",
    "        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n",
    "    \n",
    "\n",
    "        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(x_train, x_test, y_train, y_test, features, lr, epochs, log_every):\n",
    "    # set seed\n",
    "    np.random.seed(444)\n",
    "\n",
    "    # make a single linear layer\n",
    "    model = MLP(features)\n",
    "\n",
    "    # make optimizer\n",
    "    optim = GradientDescent(model.layers, lr)\n",
    "\n",
    "    # loss function\n",
    "    n_class = len(np.unique(y_train))\n",
    "    loss_fn = CrossEntropyLoss(n_class)\n",
    "\n",
    "    # evaluation function\n",
    "    eval_fn = lambda yhat, y: np.mean(np.argmax(yhat, axis=1) == y)\n",
    "\n",
    "    # begin training\n",
    "    for e in range(1, epochs+1):\n",
    "        # forward (linear layer)\n",
    "        out = model(x_train)\n",
    "        # compute cross-entropy loss and gradient w.r.t out\n",
    "        loss, dout = loss_fn(out, y_train)\n",
    "        # backward (linear layer)\n",
    "        model.backward(dout)\n",
    "        # gradient descent\n",
    "        optim.step()\n",
    "        # print log\n",
    "        if e % log_every == 0:\n",
    "            yhat = softmax(model(x_test))\n",
    "            loss = cross_entropy_loss(yhat, y_test, n_class)\n",
    "            accr = eval_fn(yhat, y_test)\n",
    "            print(f'Epochs: {e}/{epochs}, loss (test): {loss:.4f}, accuracy (test): {accr:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compute_decision_boundary_mlp(x, model, resolution):\n",
    "    x1, x2 = x[:, 0], x[:, 1]\n",
    "    x1_mesh = np.linspace(np.min(x1) - 0.1, np.max(x1) + 0.1, resolution)\n",
    "    x2_mesh = np.linspace(np.min(x2) - 0.1, np.max(x2) + 0.1, resolution)\n",
    "    x1_mesh, x2_mesh = np.meshgrid(x1_mesh, x2_mesh, indexing='xy')\n",
    "    x = np.c_[x1_mesh.ravel(), x2_mesh.ravel()]\n",
    "    y_mesh = np.argmax(softmax(model(x)), axis=1).reshape(resolution, resolution)\n",
    "    return x1_mesh, x2_mesh, y_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp = train_mlp(x_train, x_test, y_train, y_test, features=[2, 3], lr=1e-2, epochs=200, log_every=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_mesh, x2_mesh, y_mesh = compute_decision_boundary_mlp(x_test, slp, resolution=500)\n",
    "plot_spiral(x_test, y_test, x1_mesh, x2_mesh, y_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = train_mlp(x_train, x_test, y_train, y_test, features=[2, 10, 3], lr=1e-2, epochs=500, log_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_mesh, x2_mesh, y_mesh = compute_decision_boundary_mlp(x_test, mlp, resolution=500)\n",
    "plot_spiral(x_test, y_test, x1_mesh, x2_mesh, y_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mplcolors\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(data_dir, train, download, transform):\n",
    "\n",
    "    data = torchvision.datasets.MNIST(data_dir, train, download=True, transform=transform)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLZ change the directory below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "# 'data_dir' argument: path to your directory where the MNIST dataset to be downloaded\n",
    "train_data = load_mnist(data_dir='/path/to/your/dir', train=True, download=True, transform=transform)\n",
    "test_data = load_mnist(data_dir='/path/to/your/dir', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_stats(data):\n",
    "\n",
    "    number_of_images = len(data)\n",
    "\n",
    "    img_size = 'x'.join(map(str, data.data.shape[1:]))\n",
    "\n",
    "    avg_data = data.data.float().mean()\n",
    "    std_data = data.data.float().std()\n",
    "    ms_data = f'{avg_data:.2f} ({std_data:.2f})'\n",
    "\n",
    "    n_class_data, n_img_class_data = np.unique(data.targets, return_counts=True)\n",
    "    n_class_data = len(n_class_data)\n",
    "    n_img_class_data = ', '.join([f'{i}: {n:4d}' for i, n in enumerate(n_img_class_data)])\n",
    "\n",
    "    data_stats = [[number_of_images, img_size, ms_data, n_img_class_data]]\n",
    "    print(tabulate(data_stats, headers=['n img', 'img size', 'mean & stdv', 'n img per class' ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  n img  img size    mean & stdv    n img per class\n",
      "-------  ----------  -------------  ----------------------------------------------------------------------------------------\n",
      "  60000  28x28       33.32 (78.57)  0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949\n"
     ]
    }
   ],
   "source": [
    "print_data_stats(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_indices(n_class, labels):\n",
    "    indices = []\n",
    "    for i in range(n_class):\n",
    "        idx = np.where(labels == i)[0]\n",
    "        idx_selected = np.random.choice(idx, size=3)\n",
    "        indices.append(idx_selected)\n",
    "    return np.array(indices).T\n",
    "\n",
    "def plot_random_images(images, labels, n_class):\n",
    "    indices = random_indices(n_class, np.array(labels))\n",
    "    nrow, ncol = indices.shape\n",
    "\n",
    "    fig, axs = plt.subplots(nrow, ncol, figsize=(15,5), constrained_layout=True)\n",
    "    for i in range(nrow):\n",
    "      for j in range(ncol):\n",
    "        img = images[indices[i][j]]\n",
    "        axs[i][j].imshow(img, cmap='gray')\n",
    "        axs[i][j].set_xticks([])\n",
    "        axs[i][j].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_images(train_data.data, train_data.targets, n_class=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        #########################################################################################\n",
    "        # TODO: Implement __init__() function. Create a convolutional neural net using pytorch  #\n",
    "        # nn.Module. The CNN architecture is as follows:                                        #\n",
    "        #     [Layer1] -> Conv layer (in_channels=1, out_channels=6, kernel=5, stride=1)        #\n",
    "        #     [Layer2] -> ReLU layer                                                            #\n",
    "        #     [Layer3] -> MaxPool layer (kernel=2, stride=2)                                    #\n",
    "        #     [Layer4] -> Conv layer (in_channels=6, out_channels=16, kernel=2, stride=1)       #\n",
    "        #     [Layer5] -> ReLU layer                                                            #\n",
    "        #     [Layer6] -> MaxPool layer (kernel=2, stride=2)                                    #\n",
    "        #     [Layer7] -> Flatten layer                                                         #\n",
    "        #     [Layer8] -> Linear layer (in_features=400, out_features=120)                      #\n",
    "        #     [Layer9] -> ReLU layer                                                            #\n",
    "        #     [Layer10] -> Linear layer (in_features=120, out_features=84)                      #\n",
    "        #     [Layer11] -> ReLU layer                                                           #\n",
    "        #     [Layer12] -> Linear layer (in_features=84, out_features=10)                       #\n",
    "        #########################################################################################\n",
    "        # **************** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) **************** #\n",
    "\n",
    "        \n",
    "        # ***************** END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ***************** #\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, test_data, epochs, batch_size, lr):\n",
    "    # set seed\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #################################################################################\n",
    "    # TODO: Implement a training function. You MUST implement all of the following: #\n",
    "    #       (1) Load dataset (both train and test data) into pytorch DataLoader.    #\n",
    "    #           Set shuffle argument True for train data and False for test data.   #\n",
    "    #       (2) Instantiate a CNN model and ship to a device.                       #\n",
    "    #           Please use the device variable above.                               #\n",
    "    #       (3) Instantiate a sgd optimizer.                                        #\n",
    "    #       (4) Instantiate a cross-entropy loss function.                          #\n",
    "    #       (5) Implement one_step function (see the function below for details).   #\n",
    "    #################################################################################\n",
    "    # ************ START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ************ #\n",
    "\n",
    "    # TODO: (1) load dataset into dataloader\n",
    "\n",
    "\n",
    "    # TODO: (2) instantiate a CNN model and ship to device\n",
    "\n",
    "\n",
    "    # TODO: (3) instantiate a sgd optimizer\n",
    "\n",
    "\n",
    "    # TODO: (4) instantiate a cross-entropy loss function\n",
    "\n",
    "\n",
    "    # eval function\n",
    "    def calc_accuracy(yhat, y):\n",
    "        labels = torch.argmax(yhat, dim=1)\n",
    "        return (labels == y).sum().item() / y.size(0)\n",
    "\n",
    "    eval_fn = calc_accuracy\n",
    "\n",
    "    # lists to store training logs\n",
    "    train_losses, train_accrs = [], []\n",
    "    test_losses, test_accrs = [], []\n",
    "\n",
    "    # TODO: (5) Implement one_step function\n",
    "    def one_step(x, y, is_training):\n",
    "        loss, accr = None, None\n",
    "        ###################################################################################\n",
    "        # TODO: Implement step function. This function intakes x, y, and is_training and  #\n",
    "        # returns loss and accuracy. x is the input images and y is the corresponding     #\n",
    "        # class labels. is_training is a boolean argument indicating if this function is  #\n",
    "        # called in the training phase or the test phase. Note that it will be used in the#\n",
    "        # main training loop below. You MUST implement all of the following:              #\n",
    "        #       (5.1) forward propagation.                                                #\n",
    "        #       (5.2) compute cross-entropy loss and store the result in loss variable.   #\n",
    "        #       (5.3) compute accuracy and store the result in accr variable.             #\n",
    "        #       (5.4) set all gradient tensors to zero.                                   #\n",
    "        #       (5.5) compute the gradient of all leaf tensors (back propagation).        #\n",
    "        #       (5.6) update the parameters using the SGD algorithm.                      #\n",
    "        ###################################################################################\n",
    "        # ************* START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ************* #\n",
    "        \n",
    "        # TODO: (5.1) forward propagation\n",
    "\n",
    "\n",
    "        # TODO: (5.2) compute cross-entropy loss\n",
    "\n",
    "\n",
    "        # TODO: (5.3) compute accuracy\n",
    "\n",
    "\n",
    "        if is_training:\n",
    "            # TODO: (5.4) set gradient zero\n",
    "\n",
    "\n",
    "            # TODO: (5.5) compute gradients\n",
    "\n",
    "\n",
    "            # TODO: (5.6) update parameters\n",
    "\n",
    "\n",
    "        # ************** END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ************** #\n",
    "        return loss, accr\n",
    "    \n",
    "\n",
    "    # main training loop\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0.\n",
    "        train_accr = 0.\n",
    "        for i, (x, y) in enumerate(tqdm(train_loader)):\n",
    "            # ship data to device\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # NOTE: here we call the step function\n",
    "            loss, accr = one_step(x, y, is_training=True)\n",
    "            # accumulate the loss and accuracy\n",
    "            train_loss += loss\n",
    "            train_accr += accr\n",
    "        # save log\n",
    "        train_losses.append(train_loss.item()/(i+1))\n",
    "        train_accrs.append(train_accr/(i+1))\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.\n",
    "            test_accr = 0.\n",
    "            for i, (x, y) in enumerate(tqdm(test_loader)):\n",
    "                # ship data to device\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                loss, accr = one_step(x, y, is_training=False)\n",
    "                test_loss += loss\n",
    "                test_accr += accr\n",
    "            # save log\n",
    "            test_losses.append(test_loss.item()/(i+1))\n",
    "            test_accrs.append(test_accr/(i+1))\n",
    "        \n",
    "        # print log\n",
    "        log = f'Epoch: {e+1}/{epochs}, ' + \\\n",
    "              f'loss (train): {train_losses[-1]:.4f}, ' + \\\n",
    "              f'accuracy (train): {train_accrs[-1]*100:2.2f}%, ' + \\\n",
    "              f'loss (test): {test_losses[-1]:.4f}, ' + \\\n",
    "              f'accuracy (test): {test_accrs[-1]*100:2.2f}%'\n",
    "        print(log)\n",
    "    \n",
    "    print('Done.')\n",
    "\n",
    "    return train_losses, train_accrs, test_losses, test_accrs\n",
    "    # ************* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ************* #\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_accr, test_loss, test_accr = train(train_data, test_data, epochs=5, batch_size=32, lr=1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
